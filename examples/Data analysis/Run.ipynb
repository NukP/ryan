{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic data analysis and plotting.\n",
    "Update: 14 $^{th}$ Auguest 2025\n",
    "\n",
    "__Notes:__  \n",
    "Tested for the following dgbowl library version:\n",
    "- yadg: 6.1.3\n",
    "- dgpost: 2.3  \n",
    "\n",
    "- The issues with software dependencies in the previous version has been partially fixed. Now, one can pip install yadg, dgpost directly without software conflict. The order of installation matters, yadg must be installed first, fellow by dgpost.\n",
    "\n",
    "- yadg 6.1.3 includes a fix that handle file codec error. We still don't know what caused this issue, but sometime the potentiostat recorded data that cannot be decoded using utf-8 standard. This crashed yadg step in the past. Normally we either have to convert the file into .mpt or re-run the experiment completly. This new version address this issue. \n",
    "\n",
    "### New features in this version\n",
    "- __Show log__: In aa.auto_dgbowl, if show_log=True (in single CPU mode), then all log from terminal will be displayed directly in this Jupyter Notebook. This will allow ease of debugging in case there is a bug with a particular experiment folder. \n",
    "\n",
    "- __clean_temp_dir__: In aa.auto_dgbowl, if clean_temp_dir=Flase, the sliced folder in recipe/data_for_dgbowl which is the actual data folder being submitted to yadg will be preserved. Normally this folder is deleted to avoid cluttering. This option will make debugging to be easier as the recipe files are preserved in that folder. \n",
    "\n",
    "- __Abort dgpost if yadg steps failed__: In this latest version, Autoplot will abort dgpost step if yadg step fails. This will save analysis time and make debudding easier.\n",
    "\n",
    "### __How to use__\n",
    "- Put the files to be analyzed in the Data folder.\n",
    "- Click Run All button. \n",
    "- The processed files will be put in the Output folder. \n",
    "- The exported graph will be placed in Graph_Export folder. \n",
    "\n",
    "#### __If the Run All button cannot be found__\n",
    "1. Select the first cell.\n",
    "2. Control + Enter\n",
    "3. There may be a drop down menu asking Python version, choose Python 3.9. The actual version does not really matter much (This can be Python 3.9.11 or 3.9.16).\n",
    "4. After the first cell is run, the Run All button should appear next to the + Code + Markdown button on the top left corner. \n",
    "\n",
    "\n",
    "### __Supported experiment type__\n",
    " - __Single experiment__\n",
    "    - With/ without LC data.\n",
    "    - With / without temperature data\n",
    "    - With / without pressure data\n",
    "    - Flow data (with automatic removing of GC injection data points, this help reducing overshooting of FE due to inconsistent flow.)\n",
    "        - Recorded with DryCal software. \n",
    "        - Recorded with the custom Python script.\n",
    "        \n",
    "- __Multiplexing from 2 units and more__\n",
    "    - With / without temperature data\n",
    "    - With / without pressure data\n",
    "    - LC data is supported at this version (provided the LC file is placed directly inside eash unit's folder)\n",
    "    - Automatic flow data slicing and removing GC valve switching and GC injection data points. The flow data has to be recorded using the custom python script. \n",
    "    - Compatible with 1 and more GC folders / flow meter files for one multiplex experiment.\n",
    "\n",
    "### __Files requirement__\n",
    "- Please refers to an example files for a better idea on how the file naming should be. \n",
    "- **** The naming of the files have to be strictly follow the following convention to ensure the smooth running of the script.\n",
    "- Unless specified otherwise, no space can be used. Please use underscore (_).\n",
    "- Unless specified otherwise (LC data has different format), the date info has to be in the format of yyyymmdd. For example: 20230622.\n",
    "\n",
    "- __Single experiment__\n",
    "\n",
    "    - __Pressure data (optional):__ Two files are required.\n",
    "        - pressure file: \n",
    "            - In the format of yyyymmdd_XXX-pressure.csv \n",
    "            - Inside the file, ensure the columns name are UXLiquid(Read)[mbar] and UXGas(Read)[mbar] (X is the number of the unit)\n",
    "        - pressure log:\n",
    "            - In the format of yyyymmdd_XXX-pressurelog.txt\n",
    "    - __Temperature data (optional):__ One file is required.\n",
    "        - temperature file:\n",
    "            - In the format of yyyymmdd_XXX-temperature.csv \n",
    "            - The script will take column B as the cell temperature and column D as a room temperature. \n",
    "    - __Flow data (required):__ Two files are required. \n",
    "        - Flow file: \n",
    "            - In the format of yyyymmdd_XXX-flow.csv \n",
    "            - This can be from the custom script or from DryCal program. \n",
    "        - GC files folder:\n",
    "            - In the format of yyyymmdd_XXX-GC.zip\n",
    "            - This is used to remove the artifact points resulted from GC injection. We found that this usually lead to incorrect FE calculation. By default, the script will remove all flow data recorded within the time windows of 75 seconds before and after the GC injection.\n",
    "\n",
    "- __Multiplex experiment__\n",
    "    - Capable of analyzing  multiplex experiment from 2 cells onward.\n",
    "    - For a folder containing a multiplex experiment, the name of the folder must be started with 'Multiplex'\n",
    "    - The script will take anything between the first and second (if any) underscore as the experiment name: eg. Multiplex2_30p-flow-NoEDC, the exp name will be 30p-flow-NoEDC. Multiplex_20230805_30p-flow-NoEDC, exp name will be 20230805.\n",
    "\n",
    "    -__Flow data (required):__\n",
    "        - Must be in the format of collection-[sequence of unit]-flow.csv. For example, if the the said flow data stores the flow data from unit U2 and U4 (alternating) the naming would be collection-U2-U4-flow.csv. \n",
    "\n",
    "    -__GC data (required):__\n",
    "        - Must be in the format of collection-[sequence of unit]-GC.zip. For example, if the the said flow data stores the flow data from unit U2 and U4 (alternating) the naming would be collection-U2-U4-GC.zip \n",
    "    \n",
    "    -__Echem files (required):__ \n",
    "        - Echem files for each unit must be stored inside a unit folder (naming UX, where X is the unit number.)\n",
    "\n",
    "    -__Pressure data (optional):__ \n",
    "        - pressure file: \n",
    "            - In the format of yyyymmdd_XXX-pressure.csv \n",
    "            - Inside the file, ensure the columns name are UXLiquid(Read)[mbar] and UXGas(Read)[mbar] (X is the number of the unit). The script will slice take the pressure from the number in this file. Make sure there are columns from U1 to U8. \n",
    "        - pressure log:\n",
    "            - In the format of yyyymmdd_XXX-pressurelog.txt\n",
    "\n",
    "    -__Temperature data (optional):__ One file is required.\n",
    "        - temperature file:\n",
    "            - In the format of yyyymmdd_XXX-temperature.csv \n",
    "            - The temp column of each unit is in the format of Unit X Last (C). Note that here the space is used. The room temperature is in the column called RT-external Last (C). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other modules\n",
    "import warnings\n",
    "import importlib # incase we need to reload a module (for testing)\n",
    "\n",
    "#Modules for AutoDB\n",
    "from Recipe.workflow import autoanalyze as aa\n",
    "from Recipe.workflow import autoplot as ap\n",
    "from Recipe.workflow import auxiliary as aux\n",
    "from Recipe.workflow.dynamic_recipe import recifier, templates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data slicing\n",
    "- Clean 'Recipe/data_for_dgbowl' for any old data and prepare for the current data.\n",
    "- Slice data and prepare them for processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(aux)\n",
    "\n",
    "aux.clear_dir('Recipe/data_for_dgbowl')\n",
    "aux.clear_dir('Output') #!! Warning this is for development purposes only. Do not call this function when analyzing the data\n",
    "\n",
    "aux.data_slicer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yadg and dgpost\n",
    "- ### Multiprocessing\n",
    "    - __False:__\n",
    "        - The script will process the data folder by folder using only one CPU core. This method can be slower in many cases but will ensure that the sure can use a computer while running the script. \n",
    "    - __True:__\n",
    "        - The script will use ALL of the CPU cores avilable in the machine to process the data. Each core will responsible for one folder. \n",
    "        - This can greatly reduce the time required for processing the data, and the processing time will be greatly be affected by the computational power of the machine. \n",
    "        - WARNING!! Using this feature can make the machine become lag to the point where other task cannot be run and cause programs to crash. It is highly adviced to leave the machine running the script and not working on other programs. In case of large files (especially for multiplex8 experiment), the system can run out of memory (RAM), resulting in a slow down of the system. In such case, single core processing is adviced (Multiprocessing=False). \n",
    "\n",
    "- ### Mannual recipe\n",
    "    - This feature allows the user to choose whether they will manually create the recipe for yadg and dgpost themselves or will let the system to dynamically create the recipes based on the data avilability and the experiment being run. \n",
    "    - __manual_recipe=True__: The user will create a custom recipe for yadg/dgpost and put the recipes in Recipe/yadg and Recipe/dgpost\n",
    "    - __manual_recipe=False__: The user will let the system to dynamically generate the recipe for yadg / dgpost.\n",
    "        - Currently,only the dgpost recipe will be adjusted. Here are the following points that the script will look into and adjust the recipe accordingly:\n",
    "            - __Temperature__:\n",
    "                - If there is a temperature file, the recipe will read the temp. data\n",
    "                - If there is no temperature file, the recipe will put the default of 20 Celcius. \n",
    "            - __Pressure__:\n",
    "                - The recipe will extract the pressure file only if there is a pressure files. \n",
    "            - __pH__:\n",
    "                - The recipe will extract the pH from the metadata file.\n",
    "            - __LC data__:\n",
    "                - The script will generate the recipe for LC files, only if there is LC data. \n",
    "            - __Correctly assgined the charge to C for each experiment__:\n",
    "                - The charge for C wil be assigned to 4 for CO<sub>2</sub>RR and 2 for CORR for the caluclation of FE for GC. \n",
    "                - The information on the reaction being performed will be extracted from the metadata file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(aa)\n",
    "importlib.reload(recifier)\n",
    "importlib.reload(templates)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\") #Suppress UserWarning from openpyxl library\n",
    "aa.auto_dgbowl(multi_processing=False, show_log=True, manual_recipe=False, clean_temp_dir=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic graph plotting\n",
    "__Avilable variable to control the appearance of graph__\n",
    "| Parameter         | Explanation                                 | Default Value |\n",
    "|-------------------|---------------------------------------------|---------------|\n",
    "| `y_label_shift`   | Controlling the shift of the ylabel.        | -0.085        |\n",
    "| `ytick_label_size`| Controlling font size of numbers on y-axis  | 11            |\n",
    "| `xtick_label_size`| Controlling font size of numbers on x-axis  | 11            |\n",
    "| `yaxis_label_size`| Controlling font size of y-axis label.      | 11            |\n",
    "\n",
    "- To use these, simply add this to dir_make_graph function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ap) # Incase I want to re-load the module during testing.\n",
    "importlib.reload(recifier)\n",
    "importlib.reload(templates)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\") #Suppress UserWarning from openpyxl library\n",
    "\n",
    "\n",
    "ap.dir_make_graph(type='GC', format='png', save_fig=False, path_output='Graph_Export')\n",
    "ap.dir_make_graph('Output',type='LC', save_fig=False, path_output='Graph_Export')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenBis automatic upload\n",
    "- This function will automatically upload the raw data, output data onto OpenBis.\n",
    "- This function is deactivated (by commenting the script with #) on default. Only run this command (by removing # and run the cell below.) only when you are satisfied with the result and are ready to upload the experiment onto OpenBis. This is to prevent multiple duplication of experiment on OpenBis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apb.run(space_code='TEST_SPACE_PYBIS', project_code='TEST_UPLOAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoplot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
